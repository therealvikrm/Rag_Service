spring.application.name=knowItAll

# =====================================================
# Server Configuration
# =====================================================
server.port=8080
server.servlet.context-path=/

# =====================================================
# Database Configuration (H2 for Development)
# =====================================================
# H2 In-Memory Database
spring.datasource.url=jdbc:h2:mem:knowitall
spring.datasource.driverClassName=org.h2.Driver
spring.datasource.username=sa
spring.datasource.password=

# H2 Console (accessible at http://localhost:8080/h2-console)
spring.h2.console.enabled=true
spring.h2.console.path=/h2-console

# =====================================================
# JPA/Hibernate Configuration
# =====================================================
spring.jpa.database-platform=org.hibernate.dialect.H2Dialect
spring.jpa.hibernate.ddl-auto=create-drop
spring.jpa.show-sql=false
spring.jpa.properties.hibernate.format_sql=true
spring.jpa.properties.hibernate.use_sql_comments=true
spring.jpa.properties.hibernate.jdbc.batch_size=20
spring.jpa.properties.hibernate.order_inserts=true
spring.jpa.properties.hibernate.order_updates=true

# Disable Spring Boot's schema initialization (let Hibernate handle it)
spring.sql.init.mode=never

# =====================================================
# Logging Configuration
# =====================================================
logging.level.root=INFO
logging.level.com.genai.knowitall=DEBUG
logging.level.org.springframework.web=INFO
logging.level.org.hibernate.SQL=WARN
logging.level.org.hibernate.orm.jdbc.batch=WARN
logging.level.org.hibernate.orm.connections.pooling=WARN
logging.level.org.hibernate.type.descriptor.sql.BasicBinder=TRACE

# =====================================================
# LLM Provider Configuration (OpenAI)
# =====================================================
# Set OPEN_AI_KEY or OPENAI_API_KEY in your environment or in .vscode/launch.json
openai.api.key=${OPENAI_API_KEY:${OPEN_AI_KEY:}}
openai.embedding.model=${OPENAI_EMBEDDING_MODEL:text-embedding-3-small}

# =====================================================
# Vector Database Configuration (Qdrant)
# =====================================================
# Use gRPC port 6334 (REST is 6333). Java client uses gRPC.
qdrant.host=${QDRANT_HOST:localhost}
qdrant.port=${QDRANT_PORT:6334}
qdrant.rest.port=${QDRANT_REST_PORT:6333}
# Vector size must match embedding model (e.g. text-embedding-3-small = 1536)
qdrant.vector.size=${QDRANT_VECTOR_SIZE:1536}
qdrant.api.key=${QDRANT_API_KEY:}
qdrant.collection.name=${QDRANT_COLLECTION:knowitall_docs}
qdrant.timeout.seconds=${QDRANT_TIMEOUT_SEC:30}

# =====================================================
# Document Processing Configuration
# =====================================================
# Chunking parameters
doc.chunking.size.tokens=${CHUNK_SIZE_TOKENS:512}
doc.chunking.overlap.tokens=${CHUNK_OVERLAP_TOKENS:50}

# File upload configuration
spring.servlet.multipart.enabled=true
spring.servlet.multipart.max-file-size=${MAX_FILE_SIZE:10MB}
spring.servlet.multipart.max-request-size=${MAX_REQUEST_SIZE:10MB}
doc.max.file.size.bytes=${MAX_FILE_SIZE_BYTES:10485760}
doc.allowed.file.types=${ALLOWED_FILE_TYPES:application/pdf,application/vnd.openxmlformats-officedocument.wordprocessingml.document}

# Async processing configuration
spring.task.execution.pool.core-size=${ASYNC_POOL_CORE_SIZE:5}
spring.task.execution.pool.max-size=${ASYNC_POOL_MAX_SIZE:20}
spring.task.execution.pool.queue-capacity=${ASYNC_POOL_QUEUE_CAPACITY:100}
spring.task.execution.thread-name-prefix=async-doc-processor-

# Retry configuration for embedding API calls
doc.embedding.retry.max-attempts=${EMBEDDING_RETRY_MAX:3}
doc.embedding.retry.backoff-ms=${EMBEDDING_RETRY_BACKOFF:1000}

# =====================================================
# LLM Configuration (OpenAI)
# =====================================================
# Model selection: gpt-4, gpt-3.5-turbo
openai.model.name=${OPENAI_MODEL_NAME:gpt-4o-mini}

# Temperature: 0.0 (deterministic) to 1.0 (creative)
# For RAG: lower temperature (~0.7) keeps answers grounded
openai.temperature=${OPENAI_TEMPERATURE:0.7}

# Maximum tokens in LLM response
openai.max.tokens=${OPENAI_MAX_TOKENS:1000}

# =====================================================
# RAG Configuration
# =====================================================
# Number of chunks to retrieve for context
rag.retrieval.top.k=${TOP_K_RETRIEVAL:5}

# Minimum similarity score (0-1) to consider context valid
# 0.7 = balanced: good precision, won't refuse most queries
rag.confidence.threshold=${CONFIDENCE_THRESHOLD:0.7}

# Maximum tokens to include in LLM context (prevents token overflow)
rag.max.context.tokens=${MAX_CONTEXT_TOKENS:2000}

# =====================================================
# Actuator/Monitoring Configuration
# =====================================================
management.endpoints.web.exposure.include=health,info,metrics
management.endpoint.health.show-details=always
management.metrics.enable.jvm=true
management.metrics.enable.process=true


